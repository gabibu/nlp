{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq7vz0__2J7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd024d7c-a111-4f95-f9bf-d68eacec8c59"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install transformers\n",
        "!pip install livelossplot==0.5.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n",
            "Collecting livelossplot==0.5.4\n",
            "  Downloading livelossplot-0.5.4-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.4) (5.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.4) (3.2.2)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.4) (2.3.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (2.11.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (5.1.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (1.19.5)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (21.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (5.4.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (3.7.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot==0.5.4) (2.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot==0.5.4) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot==0.5.4) (1.15.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (5.0.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot==0.5.4) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->livelossplot==0.5.4) (0.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.5.4) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.5.4) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->livelossplot==0.5.4) (0.7.0)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2H9Z4iZ2XPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991a0377-10e9-454a-9e92-3518f0c5197d"
      },
      "source": [
        "from google.colab import files\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Optional\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn, utils\n",
        "from torch.utils.data import TensorDataset, RandomSampler, Dataset\n",
        "import torch\n",
        "from livelossplot import PlotLosses\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "h0uiWBjK2jtU",
        "outputId": "b3ee3fc9-10b2-4a16-893d-b7330efce5e0"
      },
      "source": [
        "#upload kaggle.json file with permission to download jigsaw-toxic-comment-classification-challenge dataset\n",
        "files.upload()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1172b13e-2728-4f18-b065-5ba4047741be\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1172b13e-2728-4f18-b065-5ba4047741be\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"gabib3b\",\"key\":\"817d7e169db4cbef867b22907320144c\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYFkqZ-42pXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e1060a1-1d43-48b1-e4f3-c03e30d6cd03"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "Downloading test_labels.csv.zip to /content\n",
            "  0% 0.00/1.46M [00:00<?, ?B/s]\n",
            "100% 1.46M/1.46M [00:00<00:00, 99.3MB/s]\n",
            "Downloading train.csv.zip to /content\n",
            " 65% 17.0M/26.3M [00:00<00:00, 36.6MB/s]\n",
            "100% 26.3M/26.3M [00:00<00:00, 75.8MB/s]\n",
            "Downloading test.csv.zip to /content\n",
            " 73% 17.0M/23.4M [00:00<00:00, 32.7MB/s]\n",
            "100% 23.4M/23.4M [00:00<00:00, 59.3MB/s]\n",
            "Downloading sample_submission.csv.zip to /content\n",
            "  0% 0.00/1.39M [00:00<?, ?B/s]\n",
            "100% 1.39M/1.39M [00:00<00:00, 197MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnbKHLMmrX5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cb5a052-ea72-4952-c30e-e1b892429a11"
      },
      "source": [
        "! ls  "
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json  sample_submission.csv.zip\ttest_labels.csv.zip\n",
            "sample_data  test.csv.zip\t\ttrain.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjfnT_W9EE-p",
        "outputId": "7a8a5896-1050-452b-b189-0eb901e99e93"
      },
      "source": [
        "!unzip train.csv.zip"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train.csv.zip\n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWqYvAjmEb34",
        "outputId": "dc680b55-1d6b-4889-eff1-7ebc81291fd8"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "kaggle.json  sample_submission.csv.zip\ttest_labels.csv.zip  train.csv.zip\n",
            "sample_data  test.csv.zip\t\ttrain.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIIqvWYAuRxa"
      },
      "source": [
        "SAMPLE_SIZE = 100000\n",
        "BERT_MODEL_NAME = 'bert-base-uncased'"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z5kTCLu3wPp"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")[['id', 'comment_text', 'toxic']].sample(SAMPLE_SIZE)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN_i6XAGgR0W",
        "outputId": "909c8f25-817e-4877-9a00-0d8a895b35a0"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100000 entries, 56117 to 499\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count   Dtype \n",
            "---  ------        --------------   ----- \n",
            " 0   id            100000 non-null  object\n",
            " 1   comment_text  100000 non-null  object\n",
            " 2   toxic         100000 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 3.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "rg4nBRckEjh0",
        "outputId": "5f590c09-b2a5-400f-bbd9-0a078e424018"
      },
      "source": [
        "df.sample(3)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>50983</th>\n",
              "      <td>885a529d66a12a53</td>\n",
              "      <td>Well Sinneed, Muze? Where are you? I will happ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>153160</th>\n",
              "      <td>986028b5b290e0a6</td>\n",
              "      <td>\"\\n\\n \"\"I haven't removed any references\"\" - g...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>152112</th>\n",
              "      <td>881c3ef28f99aa60</td>\n",
              "      <td>Fine fine, I agree to all of the above. Sorry ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id  ... toxic\n",
              "50983   885a529d66a12a53  ...     0\n",
              "153160  986028b5b290e0a6  ...     0\n",
              "152112  881c3ef28f99aa60  ...     0\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "lxkLY0FTFOUD",
        "outputId": "c536552f-c79c-4d76-ad2c-aa3b747ee975"
      },
      "source": [
        "ax = sns.countplot(x=\"toxic\", data=df)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2V7clK9x-oW",
        "outputId": "fffd118d-868a-4445-c50f-077fd0078989"
      },
      "source": [
        "df.rename(columns={'toxic': 'target'}, inplace = True)\n",
        "\n",
        "train_df, validation_df = train_test_split(df, test_size = 0.2)\n",
        "train_df.shape[0], validation_df.shape[0]"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t92_K7YnwzZM"
      },
      "source": [
        "class TextTokenizer:\n",
        "\n",
        "  MAX_NUMBER_OF_WORDS_IN_SENTENSE = 512\n",
        "\n",
        "  def __init__(self, bert_model_name):\n",
        "    self._tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "    self._cls_token = \"{} \".format(tokenizer.cls_token)\n",
        "    self._sep_token = \" {}\".format(tokenizer.sep_token)\n",
        "  \n",
        "  def text_to_tokens_ids(self, text):\n",
        "\n",
        "    marked_text = self._cls_token + text + self._sep_token\n",
        "    tokenized_text = self._tokenizer.tokenize(marked_text)[0: TextTokenizer.MAX_NUMBER_OF_WORDS_IN_SENTENSE]\n",
        "    indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    return indexed_tokens"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1hiwyptrtlf"
      },
      "source": [
        "class BaseClassifier(ABC):\n",
        "\n",
        "  @abstractmethod\n",
        "  def fit(self, df: pd.DataFrame, validation_df: Optional[pd.DataFrame]):\n",
        "    pass \n",
        "\n",
        "  @abstractmethod\n",
        "  def predict(self, df: pd.DataFrame):\n",
        "    pass \n",
        "\n",
        "  def evaluate(self, df):\n",
        "    predictions = self.predict(df)\n",
        "    targets = df['target'].tolist()\n",
        "\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(targets, predictions, average='binary')\n",
        "\n",
        "    return pd.DataFrame([(precision, recall, fscore)], columns = ['precision','recall', 'fscore'])\n"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPHmOMuPtFJy"
      },
      "source": [
        "class NearestNeiborClassifier(BaseClassifier):\n",
        "  def __init__(self, bert_model_name, hidden_layer, n_neighbors,\n",
        "               device):\n",
        "    \n",
        "    BaseClassifier.__init__(self)\n",
        "    self._hidden_layer = hidden_layer\n",
        "    self._n_neighbors = n_neighbors\n",
        "    self._model = None\n",
        "    self._device = device\n",
        "    self._bert_model = BertModel.from_pretrained(bert_model_name, \n",
        "                                                  output_hidden_states = True).to(self._device)\n",
        "    self._bert_model.eval()\n",
        "    self._tokenizer = TextTokenizer(bert_model_name)\n",
        "\n",
        "  def _text_to_sentense_embeddingds(self, text: str, embeddings_layer: int):\n",
        "    indexed_tokens = self._tokenizer.text_to_tokens_ids(text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(self._device)\n",
        "    segments_ids = [1] * len(indexed_tokens)\n",
        "    segments_tensors = torch.tensor([segments_ids]).to(self._device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = self._bert_model(tokens_tensor, segments_tensors)\n",
        "      hidden_states = outputs[2]\n",
        "      \n",
        "    return torch.mean(hidden_states[embeddings_layer], dim=1).detach().cpu().numpy().flatten() if embeddings_layer is not None else hidden_states\n",
        "  \n",
        "  def fit(self, df: pd.DataFrame, validation_df: Optional[pd.DataFrame]):\n",
        "    \n",
        "    X = df['comment_text'].progress_apply(lambda text: self._text_to_sentense_embeddingds(text, self._hidden_layer)).tolist()\n",
        "    y = df['target'].tolist()\n",
        "\n",
        "    self._model = KNeighborsClassifier(n_neighbors = self._n_neighbors)\n",
        "    self._model.fit(X, y)\n",
        "\n",
        "  def predict(self, df):\n",
        "    X = df['comment_text'].progress_apply(lambda text: self._text_to_sentense_embeddingds(text, self._hidden_layer)).tolist()\n",
        "    \n",
        "    return self._model.predict(X)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8xbBAO4Oh5j"
      },
      "source": [
        "class DatasetLoader(Dataset):\n",
        "\n",
        "    def __init__(self, df):\n",
        "        self._df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        row = self._df.iloc[idx]\n",
        "        return row['id'], row['comment_text_tokens_ids'], row['target'] if 'target' in self._df.columns else -1"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz6z_dmAOrOs"
      },
      "source": [
        "def group_rows(rows):\n",
        "\n",
        "    max_length = np.max([len(tokenes_ids) for _, tokenes_ids, _ in rows])\n",
        "\n",
        "    ids = [id for id, _, _ in rows]\n",
        "\n",
        "    tokenes_ids = torch.LongTensor([np.concatenate([tokenes_ids, np.repeat(tokenizer.pad_token_id, max_length - len(tokenes_ids))])\n",
        "                   for id, tokenes_ids, target in rows])\n",
        "\n",
        "\n",
        "    targets = [target for _, _, target in rows]\n",
        "\n",
        "    masks = torch.LongTensor([np.concatenate([np.repeat(1, len(tokenes_ids)), np.repeat(0, max_length - len(tokenes_ids))])\n",
        "          for _, tokenes_ids, _ in rows])\n",
        "    \n",
        "    return ids, tokenes_ids, masks, torch.FloatTensor(targets).reshape(-1, 1)\n"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRFYXTYIO601"
      },
      "source": [
        "class RelevanceModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model_name, hidden_layer, device):\n",
        "\n",
        "        super(RelevanceModel, self).__init__()\n",
        "\n",
        "        self._hidden_layer= hidden_layer\n",
        "        self._bert_model = BertModel.from_pretrained(bert_model_name, \n",
        "                                     output_hidden_states = True).to(device)\n",
        "\n",
        "        self._bert_model.eval()\n",
        "\n",
        "        for param in self._bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.af1 = nn.LeakyReLU()\n",
        "        self.fc1 = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, tokens, attention_mask):\n",
        "        \n",
        "        hidden_states = self._bert_model(tokens, attention_mask)[2]\n",
        "\n",
        "        sentense_embeddingds = torch.mean(hidden_states[self._hidden_layer], dim=1)\n",
        "\n",
        "        out = self.af1(sentense_embeddingds)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      \n",
        "        initrange = 0.5\n",
        "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc1.bias.data.zero_()\n",
        "        return self"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9jQmEZjPJU_"
      },
      "source": [
        "class DenseLayerOnBertModel(BaseClassifier):\n",
        "\n",
        "    def __init__(self, bert_model_name, hidden_layer, batch_size,\n",
        "                 prediction_threshold, device, learnning_rate = 0.001,\n",
        "                 number_of_epocs = 1000,\n",
        "                 verbose_eval = False):\n",
        "        BaseClassifier.__init__(self)\n",
        "        self._device = device\n",
        "        bert.to(self._device)\n",
        "        self._model  = RelevanceModel(bert_model_name, hidden_layer, self._device).reset_parameters()\n",
        "        self._model.to(self._device)\n",
        "        \n",
        "        self._batch_size = batch_size\n",
        "        self._prediction_threshold = prediction_threshold\n",
        "        self._learnning_rate = learnning_rate\n",
        "        self._number_of_epocs = number_of_epocs\n",
        "        self._tokenizer = TextTokenizer(bert_model_name)\n",
        "        self._verbose_eval = verbose_eval\n",
        "        \n",
        "\n",
        "\n",
        "    def _calc_positive_weight(self, df: pd.DataFrame) -> float:\n",
        "      weights = compute_class_weight('balanced', np.unique(train_df.target),  train_df.target)\n",
        "      pos_weight = weights[1]/weights[0]\n",
        "\n",
        "      return pos_weight   \n",
        "\n",
        "\n",
        "    def _preprocessing(self, df: pd.DataFrame):\n",
        "      df['comment_text_tokens_ids'] = df['comment_text'].progress_apply(lambda text: self._tokenizer.text_to_tokens_ids(text))\n",
        "\n",
        "    def _run_batch(self, train_data_loader, loss_fn, optimizer):\n",
        "\n",
        "      self._model.train()\n",
        "\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for ids, tokends_ids, attention_mask, targets in train_data_loader:\n",
        "                \n",
        "        tokends_ids = tokends_ids.to(self._device)\n",
        "        attention_mask = attention_mask.to(self._device)\n",
        "        targets = targets.to(self._device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = self._model(tokends_ids, attention_mask)\n",
        "        loss = loss_fn(output, targets)\n",
        "    \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "      \n",
        "      return total_loss/len(train_data_loader)\n",
        "\n",
        "    def _evaluation_loss(self, data_loader, loss_fn):\n",
        "      self._model.eval()\n",
        "\n",
        "      total_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for ids, tokends_ids, attention_mask, targets in data_loader:\n",
        "          tokends_ids = tokends_ids.to(self._device)\n",
        "          attention_mask = attention_mask.to(self._device)\n",
        "          targets = targets.to(self._device)\n",
        "          output = self._model(tokends_ids, attention_mask)\n",
        "          loss = loss_fn(output, targets)\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      return total_loss/len(data_loader)\n",
        "\n",
        "\n",
        "    def fit(self, df: pd.DataFrame, valiodation_df: Optional[pd.DataFrame]):\n",
        "        positive_weight = self._calc_positive_weight(df)\n",
        "        pos_weights_tensor = torch.Tensor([positive_weight])\n",
        "       \n",
        "        self._preprocessing(df)\n",
        "        train_dataset = DatasetLoader(df)\n",
        "        sampler = RandomSampler(train_dataset)\n",
        "\n",
        "        train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                        batch_size=self._batch_size,\n",
        "                                                        shuffle=False,\n",
        "                                                        num_workers=4,\n",
        "                                                        drop_last=False,\n",
        "                                                        sampler=sampler,\n",
        "                                                        collate_fn=group_rows)\n",
        "\n",
        "\n",
        "        validation_data_loader = None\n",
        "        if self._verbose_eval and valiodation_df is not None:\n",
        "          self._preprocessing(valiodation_df)\n",
        "          validation_dataset = DatasetLoader(valiodation_df)\n",
        "\n",
        "          validation_data_loader = torch.utils.data.DataLoader(validation_dataset,\n",
        "                                                        batch_size=self._batch_size,\n",
        "                                                        shuffle=False,\n",
        "                                                        num_workers=4,\n",
        "                                                        drop_last=False,\n",
        "                                                        collate_fn=group_rows)\n",
        "          \n",
        "\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), lr=self._learnning_rate)\n",
        "      \n",
        "        loss_fn  = nn.BCEWithLogitsLoss(pos_weight = pos_weights_tensor).to(device)\n",
        "\n",
        "        liveloss = PlotLosses() if self._verbose_eval else None\n",
        "        for batch in range(self._number_of_epocs):\n",
        "          train_batch = self._run_batch(train_data_loader, loss_fn, optimizer)\n",
        "\n",
        "          validatiob_loss = None\n",
        "          if validation_data_loader is not None:\n",
        "            validatiob_loss = self._evaluation_loss(validation_data_loader, loss_fn)\n",
        "          \n",
        "          plot_update = {'train_loss': train_batch}\n",
        "\n",
        "          if validatiob_loss is not None:\n",
        "            plot_update['validation_loss'] = validatiob_loss\n",
        "\n",
        "          if liveloss is not None:\n",
        "            \n",
        "            liveloss.update(plot_update)\n",
        "            liveloss.draw()\n",
        "\n",
        "    def predict(self, df: pd.DataFrame):\n",
        "\n",
        "      self._preprocessing(df)\n",
        "      \n",
        "      self._model.eval()\n",
        "\n",
        "      predict_dataset_loader = DatasetLoader(df)\n",
        "\n",
        "      predict_data_loader = torch.utils.data.DataLoader(predict_dataset_loader,\n",
        "                                                              batch_size=self._batch_size,\n",
        "                                                              shuffle=False,\n",
        "                                                              num_workers=4,\n",
        "                                                              drop_last=False,\n",
        "                                                              collate_fn=group_rows)\n",
        "      with torch.no_grad():\n",
        "\n",
        "        predictions = {}\n",
        "        \n",
        "        for ids, tokends_ids, attention_mask, _ in predict_data_loader:\n",
        "          tokends_ids = tokends_ids.to(device)\n",
        "          attention_mask = attention_mask.to(device)\n",
        "          \n",
        "          logit = self._model(tokends_ids, attention_mask)\n",
        "\n",
        "          probs = torch.sigmoid(logit).detach().cpu().numpy().flatten()\n",
        "          batch_predictions = probs > self._prediction_threshold\n",
        "\n",
        "          for id, prediction in zip(ids, batch_predictions):\n",
        "            predictions[id] = prediction \n",
        "          \n",
        "      return df['id'].apply(lambda id: predictions[id]).tolist()\n",
        "      "
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ0LyfuA6_lj"
      },
      "source": [
        "** NearestNeiborClassifier Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woWgCg3OPM1X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "99e71db6-08c7-45f6-e91b-97fa88cabea4"
      },
      "source": [
        "test_nn_classifier = train_df.sample(1000)\n",
        "nearest_neibor_classifier = NearestNeiborClassifier(bert_model_name = BERT_MODEL_NAME, \n",
        "                                                     hidden_layer = -1,\n",
        "                                                     n_neighbors = 5,\n",
        "                                                    device = device)\n",
        "\n",
        "nearest_neibor_classifier.fit(test_nn_classifier, None)\n",
        "nearest_neibor_classifier.evaluate(test_nn_classifier)"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 1000/1000 [00:12<00:00, 83.19it/s]\n",
            "100%|██████████| 1000/1000 [00:11<00:00, 83.83it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>fscore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.72549</td>\n",
              "      <td>0.397849</td>\n",
              "      <td>0.513889</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   precision    recall    fscore\n",
              "0    0.72549  0.397849  0.513889"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN04zBCu7JpE"
      },
      "source": [
        "**DenseLayerOnBertModel Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRO46jR2zGZv",
        "outputId": "ff1d47a1-1d2e-4809-d40b-f26ee30e3b83"
      },
      "source": [
        "dense_model_classifier = DenseLayerOnBertModel(bert_model_name = BERT_MODEL_NAME, \n",
        "                                               hidden_layer = -1,\n",
        "                                               batch_size = 32,\n",
        "                                               prediction_threshold = 0.65,\n",
        "                                               device = device, \n",
        "                                               learnning_rate= 0.001,\n",
        "                                               number_of_epocs = 5,\n",
        "                                               verbose_eval = True)\n",
        "\n",
        "\n",
        "\n",
        "dense_model_classifier.fit(train_df.sample(500), validation_df.sample(100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 500/500 [00:00<00:00, 590.87it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 100/100 [00:00<00:00, 631.38it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnTOnjJxwMQU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd2BRTnoszmH",
        "outputId": "cf5e3146-5401-4bea-ba95-a4c77dea8f7e"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "PREDICTION_THRESHOLD = 0.65 \n",
        "N_NEIGHBORS = 3\n",
        "NUMBER_OF_EPOCS = 5\n",
        "EMBEDDING_LAYERS_TO_CHECK = [9,10, 11, 12]\n",
        "scores_dfs = []\n",
        "\n",
        "train_df = train_df.sample(2000)\n",
        "validation_df = validation_df.sample(200)\n",
        "\n",
        "for hidden_layer in EMBEDDING_LAYERS_TO_CHECK:\n",
        "\n",
        "  print('running on hidden layer {}'.format(hidden_layer))\n",
        "\n",
        "  deep_model = DeepModel(bert, tokenizer, hidden_layer, BATCH_SIZE, PREDICTION_THRESHOLD, \n",
        "                         number_of_epocs = NUMBER_OF_EPOCS)\n",
        "\n",
        "  deep_model.fit(train_df)\n",
        "  scores_df = deep_model.evaluate(validation_df)\n",
        "  scores_df['model'] = 'Deep'\n",
        "  scores_df['hidden_layer'] = hidden_layer\n",
        "  scores_dfs.append(scores_df)\n",
        "  \n",
        "\n",
        "  nearestNeiborClassifier = NearestNeiborClassifier(bert, tokenizer, hidden_layer, N_NEIGHBORS)\n",
        "  nearestNeiborClassifier.fit(train_df)\n",
        "  scores_df = nearestNeiborClassifier.evaluate(validation_df)\n",
        "\n",
        "  scores_df['model'] = 'NearestNeibor'\n",
        "  scores_df['hidden_layer'] = hidden_layer\n",
        "  scores_dfs.append(scores_df)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:03<00:00, 583.10it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0 loss 0.044949567258358004\n",
            "batch 1 loss 0.037968938887119295\n",
            "batch 2 loss 0.03299155059456825\n",
            "batch 3 loss 0.030061348855495454\n",
            "batch 4 loss 0.027999427273869514\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 560.66it/s]\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 81.97it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 82.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:03<00:00, 609.56it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0 loss 0.04609228113293648\n",
            "batch 1 loss 0.03749821212887764\n",
            "batch 2 loss 0.0317545223236084\n",
            "batch 3 loss 0.028931003004312516\n",
            "batch 4 loss 0.026856014490127565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 581.54it/s]\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 81.35it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 79.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:03<00:00, 596.46it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0 loss 0.045928227841854095\n",
            "batch 1 loss 0.034758240193128585\n",
            "batch 2 loss 0.030097644075751304\n",
            "batch 3 loss 0.02676514258980751\n",
            "batch 4 loss 0.024005764037370682\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 562.95it/s]\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.66it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 81.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [00:03<00:00, 615.24it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch 0 loss 0.04103910520672798\n",
            "batch 1 loss 0.03447990986704826\n",
            "batch 2 loss 0.030801570028066636\n",
            "batch 3 loss 0.028551340699195862\n",
            "batch 4 loss 0.026590044111013414\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 200/200 [00:00<00:00, 585.31it/s]\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.31it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 77.37it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "PRlrbL0uO0oB",
        "outputId": "392174ca-ea37-4730-b14a-1a3a91734e05"
      },
      "source": [
        "pd.concat(scores_dfs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>fscore</th>\n",
              "      <th>model</th>\n",
              "      <th>hidden_layer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>Deep</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.523810</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>Deep</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.363636</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>Deep</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.645161</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.458333</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.564103</td>\n",
              "      <td>Deep</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   precision    recall    fscore          model  hidden_layer\n",
              "0   0.400000  0.800000  0.533333           Deep             9\n",
              "0   0.611111  0.733333  0.666667  NearestNeibor             9\n",
              "0   0.523810  0.733333  0.611111           Deep            10\n",
              "0   0.666667  0.666667  0.666667  NearestNeibor            10\n",
              "0   0.363636  0.800000  0.500000           Deep            11\n",
              "0   0.625000  0.666667  0.645161  NearestNeibor            11\n",
              "0   0.458333  0.733333  0.564103           Deep            12\n",
              "0   0.733333  0.733333  0.733333  NearestNeibor            12"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk7ITAnaPU7P"
      },
      "source": [
        ""
      ]
    }
  ]
}