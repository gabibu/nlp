{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nlp_final.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cq7vz0__2J7K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd024d7c-a111-4f95-f9bf-d68eacec8c59"
      },
      "source": [
        "!pip install -q kaggle\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install transformers\n",
        "!pip install livelossplot==0.5.4"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.0)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl (2.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.8 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting huggingface-hub>=0.0.12\n",
            "  Downloading huggingface_hub-0.0.16-py3-none-any.whl (50 kB)\n",
            "\u001b[K     |████████████████████████████████| 50 kB 8.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 55.9 MB/s \n",
            "\u001b[?25hCollecting pyyaml>=5.1\n",
            "  Downloading PyYAML-5.4.1-cp37-cp37m-manylinux1_x86_64.whl (636 kB)\n",
            "\u001b[K     |████████████████████████████████| 636 kB 63.8 MB/s \n",
            "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 48.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Installing collected packages: tokenizers, sacremoses, pyyaml, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.0.16 pyyaml-5.4.1 sacremoses-0.0.45 tokenizers-0.10.3 transformers-4.10.0\n",
            "Collecting livelossplot==0.5.4\n",
            "  Downloading livelossplot-0.5.4-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.4) (5.5.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.4) (3.2.2)\n",
            "Requirement already satisfied: bokeh in /usr/local/lib/python3.7/dist-packages (from livelossplot==0.5.4) (2.3.3)\n",
            "Requirement already satisfied: Jinja2>=2.9 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (2.11.3)\n",
            "Requirement already satisfied: tornado>=5.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (5.1.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (1.19.5)\n",
            "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (21.0)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (5.4.1)\n",
            "Requirement already satisfied: pillow>=7.1.0 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/dist-packages (from bokeh->livelossplot==0.5.4) (3.7.4.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2>=2.9->bokeh->livelossplot==0.5.4) (2.0.1)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=16.8->bokeh->livelossplot==0.5.4) (2.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->bokeh->livelossplot==0.5.4) (1.15.0)\n",
            "Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (4.8.0)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (57.4.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (4.4.2)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (2.6.1)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.4 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (1.0.18)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (5.0.5)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->livelossplot==0.5.4) (0.7.5)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.0.0,>=1.0.4->ipython->livelossplot==0.5.4) (0.2.5)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.7/dist-packages (from traitlets>=4.2->ipython->livelossplot==0.5.4) (0.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.5.4) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->livelossplot==0.5.4) (1.3.1)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect->ipython->livelossplot==0.5.4) (0.7.0)\n",
            "Installing collected packages: livelossplot\n",
            "Successfully installed livelossplot-0.5.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2H9Z4iZ2XPb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34e9e9b4-5c68-441e-9b32-1945a8d28ef6"
      },
      "source": [
        "from google.colab import files\n",
        "from abc import ABC, abstractmethod\n",
        "from typing import List, Optional\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch import nn, utils\n",
        "from torch.utils.data import TensorDataset, RandomSampler, Dataset\n",
        "import torch\n",
        "from livelossplot import PlotLosses\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "id": "h0uiWBjK2jtU",
        "outputId": "4bb306b5-4bae-4835-dd49-a59df837d078"
      },
      "source": [
        "#upload your kaggle.json file with permission to download jigsaw-toxic-comment-classification-challenge dataset\n",
        "files.upload()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1fa52166-f684-48b8-a4d5-ba93d332af99\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1fa52166-f684-48b8-a4d5-ba93d332af99\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle (1).json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"gabib3b\",\"key\":\"817d7e169db4cbef867b22907320144c\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYFkqZ-42pXt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a2c6e8f-0476-432f-85fa-02501022f251"
      },
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle competitions download -c jigsaw-toxic-comment-classification-challenge"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/root/.kaggle’: File exists\n",
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.4)\n",
            "train.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test_labels.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "sample_submission.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
            "test.csv.zip: Skipping, found more recently modified local copy (use --force to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FnbKHLMmrX5J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d1a6178-69b5-46cd-9b7d-47b636069128"
      },
      "source": [
        "! ls  "
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'kaggle (1).json'   sample_submission.csv.zip   train.csv\n",
            " kaggle.json\t    test.csv.zip\t        train.csv.zip\n",
            " sample_data\t    test_labels.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjfnT_W9EE-p",
        "outputId": "3bed91b7-67a1-48a4-8d33-91ccc123dfed"
      },
      "source": [
        "!unzip train.csv.zip"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train.csv.zip\n",
            "replace train.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: y\n",
            "  inflating: train.csv               \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWqYvAjmEb34",
        "outputId": "211195bc-b8a1-44a3-d775-60c05fed79b6"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'kaggle (1).json'   sample_submission.csv.zip   train.csv\n",
            " kaggle.json\t    test.csv.zip\t        train.csv.zip\n",
            " sample_data\t    test_labels.csv.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIIqvWYAuRxa"
      },
      "source": [
        "SAMPLE_SIZE = 100000\n",
        "BERT_MODEL_NAME = 'bert-base-uncased'"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z5kTCLu3wPp"
      },
      "source": [
        "df = pd.read_csv(\"train.csv\")[['id', 'comment_text', 'toxic']].sample(SAMPLE_SIZE)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bN_i6XAGgR0W",
        "outputId": "e13ebea1-a82c-400f-d093-42784b98f3d1"
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 100000 entries, 99147 to 8937\n",
            "Data columns (total 3 columns):\n",
            " #   Column        Non-Null Count   Dtype \n",
            "---  ------        --------------   ----- \n",
            " 0   id            100000 non-null  object\n",
            " 1   comment_text  100000 non-null  object\n",
            " 2   toxic         100000 non-null  int64 \n",
            "dtypes: int64(1), object(2)\n",
            "memory usage: 3.1+ MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "rg4nBRckEjh0",
        "outputId": "d0193c59-ff68-4dfd-8804-e9b817c02ba2"
      },
      "source": [
        "df.sample(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>comment_text</th>\n",
              "      <th>toxic</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>105527</th>\n",
              "      <td>34a26bbfebc3b52a</td>\n",
              "      <td>Regarding edits made during October 11 2006 (U...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>119624</th>\n",
              "      <td>7f9309dbffd8e857</td>\n",
              "      <td>Dear Baba: Your words are wise. But I am sorry...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96257</th>\n",
              "      <td>02d675ba0e8d3ee6</td>\n",
              "      <td>How about visiting Geoffrey Hinton once? Thank...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                      id  ... toxic\n",
              "105527  34a26bbfebc3b52a  ...     0\n",
              "119624  7f9309dbffd8e857  ...     0\n",
              "96257   02d675ba0e8d3ee6  ...     0\n",
              "\n",
              "[3 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "lxkLY0FTFOUD",
        "outputId": "af514cdd-ddc4-4daf-9d0c-9fac3d57d6d0"
      },
      "source": [
        "ax = sns.countplot(x=\"toxic\", data=df)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I2V7clK9x-oW",
        "outputId": "a9ccf910-d937-4db4-d899-9a3953b8d4ed"
      },
      "source": [
        "df.rename(columns={'toxic': 'target'}, inplace = True)\n",
        "\n",
        "train_df, validation_df = train_test_split(df, test_size = 0.2)\n",
        "train_df.shape[0], validation_df.shape[0]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(80000, 20000)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t92_K7YnwzZM"
      },
      "source": [
        "class TextTokenizer:\n",
        "\n",
        "  MAX_NUMBER_OF_WORDS_IN_SENTENSE = 512\n",
        "\n",
        "  def __init__(self, bert_model_name):\n",
        "    self._tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
        "    self._cls_token = \"{} \".format(self._tokenizer.cls_token)\n",
        "    self._sep_token = \" {}\".format(self._tokenizer.sep_token)\n",
        "  \n",
        "  def text_to_tokens_ids(self, text):\n",
        "\n",
        "    marked_text = self._cls_token + text + self._sep_token\n",
        "    tokenized_text = self._tokenizer.tokenize(marked_text)[0: TextTokenizer.MAX_NUMBER_OF_WORDS_IN_SENTENSE]\n",
        "    indexed_tokens = self._tokenizer.convert_tokens_to_ids(tokenized_text)\n",
        "\n",
        "    return indexed_tokens\n",
        "  \n",
        "  @property\n",
        "  def pad_token_id(self):\n",
        "    return self._tokenizer.pad_token_id"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1hiwyptrtlf"
      },
      "source": [
        "class BaseClassifier(ABC):\n",
        "\n",
        "  @abstractmethod\n",
        "  def fit(self, df: pd.DataFrame, validation_df: Optional[pd.DataFrame]):\n",
        "    pass \n",
        "\n",
        "  @abstractmethod\n",
        "  def predict(self, df: pd.DataFrame):\n",
        "    pass \n",
        "\n",
        "  def evaluate(self, df: pd.DataFrame):\n",
        "    predictions = self.predict(df)\n",
        "    targets = df['target'].tolist()\n",
        "\n",
        "    precision, recall, fscore, _ = precision_recall_fscore_support(targets, predictions, average='binary')\n",
        "\n",
        "    return pd.DataFrame([(precision, recall, fscore)], columns = ['precision','recall', 'fscore'])\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SPHmOMuPtFJy"
      },
      "source": [
        "class NearestNeiborClassifier(BaseClassifier):\n",
        "\n",
        "  def __init__(self, bert_model_name: str, hidden_layer: int, n_neighbors : int,\n",
        "               device: torch.device):\n",
        "    \n",
        "    BaseClassifier.__init__(self)\n",
        "    self._hidden_layer = hidden_layer\n",
        "    self._n_neighbors = n_neighbors\n",
        "    self._model = None\n",
        "    self._device = device\n",
        "    self._bert_model = BertModel.from_pretrained(bert_model_name, \n",
        "                                                  output_hidden_states = True).to(self._device)\n",
        "    self._bert_model.eval()\n",
        "    self._tokenizer = TextTokenizer(bert_model_name)\n",
        "\n",
        "  def _text_to_sentense_embeddingds(self, text: str, hidden_layer: int):\n",
        "    indexed_tokens = self._tokenizer.text_to_tokens_ids(text)\n",
        "    tokens_tensor = torch.tensor([indexed_tokens]).to(self._device)\n",
        "    segments_ids = [1] * len(indexed_tokens)\n",
        "    segments_tensors = torch.tensor([segments_ids]).to(self._device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = self._bert_model(tokens_tensor, segments_tensors)\n",
        "      hidden_states = outputs[2]\n",
        "      \n",
        "    return torch.mean(hidden_states[hidden_layer], dim=1).detach().cpu().numpy().flatten() if hidden_layer is not None else hidden_states\n",
        "  \n",
        "  def fit(self, df: pd.DataFrame, validation_df: Optional[pd.DataFrame]):\n",
        "    \n",
        "    X = df['comment_text'].progress_apply(lambda text: self._text_to_sentense_embeddingds(text, self._hidden_layer)).tolist()\n",
        "    y = df['target'].tolist()\n",
        "\n",
        "    self._model = KNeighborsClassifier(n_neighbors = self._n_neighbors)\n",
        "    self._model.fit(X, y)\n",
        "\n",
        "  def predict(self, df: pd.DataFrame):\n",
        "    X = df['comment_text'].progress_apply(lambda text: self._text_to_sentense_embeddingds(text, self._hidden_layer)).tolist()\n",
        "    \n",
        "    return self._model.predict(X)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q8xbBAO4Oh5j"
      },
      "source": [
        "class DatasetLoader(Dataset):\n",
        "\n",
        "    def __init__(self, df: pd.DataFrame):\n",
        "        self._df = df\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self._df.iloc[idx]\n",
        "        \n",
        "        return row['id'], row['comment_text_tokens_ids'], row['target'] if 'target' in self._df.columns else -1"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xz6z_dmAOrOs"
      },
      "source": [
        "# def group_rows(rows):\n",
        "\n",
        "#     max_length = np.max([len(tokenes_ids) for _, tokenes_ids, _ in rows])\n",
        "\n",
        "#     ids = [id for id, _, _ in rows]\n",
        "\n",
        "#     tokenes_ids = torch.LongTensor([np.concatenate([tokenes_ids, np.repeat(tokenizer.pad_token_id, max_length - len(tokenes_ids))])\n",
        "#                    for id, tokenes_ids, target in rows])\n",
        "\n",
        "\n",
        "#     targets = [target for _, _, target in rows]\n",
        "\n",
        "#     masks = torch.LongTensor([np.concatenate([np.repeat(1, len(tokenes_ids)), np.repeat(0, max_length - len(tokenes_ids))])\n",
        "#           for _, tokenes_ids, _ in rows])\n",
        "    \n",
        "#     return ids, tokenes_ids, masks, torch.FloatTensor(targets).reshape(-1, 1)\n"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YRFYXTYIO601"
      },
      "source": [
        "class RelevanceModel(nn.Module):\n",
        "\n",
        "    def __init__(self, bert_model_name: str, hidden_layer: int, device: torch.device):\n",
        "\n",
        "        super(RelevanceModel, self).__init__()\n",
        "\n",
        "        self._hidden_layer= hidden_layer\n",
        "        self._bert_model = BertModel.from_pretrained(bert_model_name, \n",
        "                                     output_hidden_states = True).to(device)\n",
        "\n",
        "        self._bert_model.eval()\n",
        "\n",
        "        for param in self._bert_model.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        self.af1 = nn.LeakyReLU()\n",
        "        self.fc1 = nn.Linear(768, 1)\n",
        "\n",
        "    def forward(self, tokens, attention_mask):\n",
        "        \n",
        "        hidden_states = self._bert_model(tokens, attention_mask)[2]\n",
        "\n",
        "        sentense_embeddingds = torch.mean(hidden_states[self._hidden_layer], dim=1)\n",
        "\n",
        "        out = self.af1(sentense_embeddingds)\n",
        "\n",
        "        out = self.fc1(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "    def reset_parameters(self):\n",
        "      \n",
        "        initrange = 0.5\n",
        "        self.fc1.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc1.bias.data.zero_()\n",
        "        return self"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9jQmEZjPJU_"
      },
      "source": [
        "class DenseLayerOnBertModel(BaseClassifier):\n",
        "\n",
        "    def __init__(self, bert_model_name: str, hidden_layer: int, batch_size: int,\n",
        "                 prediction_threshold: float, device, learnning_rate: float = 0.001,\n",
        "                 number_of_epocs: int = 1000,\n",
        "                 verbose_eval: bool = False):\n",
        "        BaseClassifier.__init__(self)\n",
        "        self._device = device\n",
        "        self._model  = RelevanceModel(bert_model_name, hidden_layer, self._device).reset_parameters()\n",
        "        self._model.to(self._device)\n",
        "        \n",
        "        self._batch_size = batch_size\n",
        "        self._prediction_threshold = prediction_threshold\n",
        "        self._learnning_rate = learnning_rate\n",
        "        self._number_of_epocs = number_of_epocs\n",
        "        self._tokenizer = TextTokenizer(bert_model_name)\n",
        "        self._verbose_eval = verbose_eval\n",
        "        \n",
        "\n",
        "    def group_rows(self, rows):\n",
        "\n",
        "      max_length = np.max([len(tokenes_ids) for _, tokenes_ids, _ in rows])\n",
        "\n",
        "      ids = [id for id, _, _ in rows]\n",
        "\n",
        "      tokenes_ids = torch.LongTensor([np.concatenate([tokenes_ids, np.repeat(self._tokenizer.pad_token_id, max_length - len(tokenes_ids))])\n",
        "                    for id, tokenes_ids, target in rows])\n",
        "\n",
        "\n",
        "      targets = [target for _, _, target in rows]\n",
        "\n",
        "      masks = torch.LongTensor([np.concatenate([np.repeat(1, len(tokenes_ids)), np.repeat(0, max_length - len(tokenes_ids))])\n",
        "            for _, tokenes_ids, _ in rows])\n",
        "    \n",
        "      return ids, tokenes_ids, masks, torch.FloatTensor(targets).reshape(-1, 1)\n",
        "\n",
        "    def _calc_positive_weight(self, df: pd.DataFrame) -> float:\n",
        "      weights = compute_class_weight('balanced', np.unique(train_df.target),  train_df.target)\n",
        "      pos_weight = weights[1]/weights[0]\n",
        "\n",
        "      return pos_weight   \n",
        "\n",
        "\n",
        "    def _preprocessing(self, df: pd.DataFrame):\n",
        "      df['comment_text_tokens_ids'] = df['comment_text'].progress_apply(lambda text: self._tokenizer.text_to_tokens_ids(text))\n",
        "\n",
        "    def _run_batch(self, train_data_loader, loss_fn, optimizer):\n",
        "\n",
        "      self._model.train()\n",
        "\n",
        "      total_loss = 0.0\n",
        "\n",
        "      for ids, tokends_ids, attention_mask, targets in train_data_loader:\n",
        "                \n",
        "        tokends_ids = tokends_ids.to(self._device)\n",
        "        attention_mask = attention_mask.to(self._device)\n",
        "        targets = targets.to(self._device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = self._model(tokends_ids, attention_mask)\n",
        "        loss = loss_fn(output, targets)\n",
        "    \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "      \n",
        "      return total_loss/len(train_data_loader)\n",
        "\n",
        "    def _evaluation_loss(self, data_loader, loss_fn):\n",
        "      self._model.eval()\n",
        "\n",
        "      total_loss = 0.0\n",
        "\n",
        "      with torch.no_grad():\n",
        "        for ids, tokends_ids, attention_mask, targets in data_loader:\n",
        "          tokends_ids = tokends_ids.to(self._device)\n",
        "          attention_mask = attention_mask.to(self._device)\n",
        "          targets = targets.to(self._device)\n",
        "          output = self._model(tokends_ids, attention_mask)\n",
        "          loss = loss_fn(output, targets)\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      return total_loss/len(data_loader)\n",
        "\n",
        "\n",
        "    def fit(self, df: pd.DataFrame, valiodation_df: Optional[pd.DataFrame]):\n",
        "        positive_weight = self._calc_positive_weight(df)\n",
        "        pos_weights_tensor = torch.Tensor([positive_weight])\n",
        "       \n",
        "        self._preprocessing(df)\n",
        "        train_dataset = DatasetLoader(df)\n",
        "        sampler = RandomSampler(train_dataset)\n",
        "\n",
        "        train_data_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                                        batch_size=self._batch_size,\n",
        "                                                        shuffle=False,\n",
        "                                                        num_workers=4,\n",
        "                                                        drop_last=False,\n",
        "                                                        sampler=sampler,\n",
        "                                                        collate_fn= self.group_rows)\n",
        "\n",
        "\n",
        "        validation_data_loader = None\n",
        "        if self._verbose_eval and valiodation_df is not None:\n",
        "          self._preprocessing(valiodation_df)\n",
        "          validation_dataset = DatasetLoader(valiodation_df)\n",
        "\n",
        "          validation_data_loader = torch.utils.data.DataLoader(validation_dataset,\n",
        "                                                        batch_size=self._batch_size,\n",
        "                                                        shuffle=False,\n",
        "                                                        num_workers=4,\n",
        "                                                        drop_last=False,\n",
        "                                                        collate_fn=group_rows)\n",
        "          \n",
        "\n",
        "        optimizer = torch.optim.Adam(self._model.parameters(), lr=self._learnning_rate)\n",
        "      \n",
        "        loss_fn  = nn.BCEWithLogitsLoss(pos_weight = pos_weights_tensor).to(device)\n",
        "\n",
        "        liveloss = PlotLosses() if self._verbose_eval else None\n",
        "        for batch in range(self._number_of_epocs):\n",
        "          train_batch = self._run_batch(train_data_loader, loss_fn, optimizer)\n",
        "\n",
        "          validatiob_loss = None\n",
        "          if validation_data_loader is not None:\n",
        "            validatiob_loss = self._evaluation_loss(validation_data_loader, loss_fn)\n",
        "          \n",
        "          plot_update = {'train_loss': train_batch}\n",
        "\n",
        "          if validatiob_loss is not None:\n",
        "            plot_update['validation_loss'] = validatiob_loss\n",
        "\n",
        "          if liveloss is not None:\n",
        "            \n",
        "            liveloss.update(plot_update)\n",
        "            liveloss.draw()\n",
        "\n",
        "    def predict(self, df: pd.DataFrame):\n",
        "\n",
        "      self._preprocessing(df)\n",
        "      \n",
        "      self._model.eval()\n",
        "\n",
        "      predict_dataset_loader = DatasetLoader(df)\n",
        "\n",
        "      predict_data_loader = torch.utils.data.DataLoader(predict_dataset_loader,\n",
        "                                                              batch_size=self._batch_size,\n",
        "                                                              shuffle=False,\n",
        "                                                              num_workers=4,\n",
        "                                                              drop_last=False,\n",
        "                                                              collate_fn=group_rows)\n",
        "      with torch.no_grad():\n",
        "\n",
        "        predictions = {}\n",
        "        \n",
        "        for ids, tokends_ids, attention_mask, _ in predict_data_loader:\n",
        "          tokends_ids = tokends_ids.to(device)\n",
        "          attention_mask = attention_mask.to(device)\n",
        "          \n",
        "          logit = self._model(tokends_ids, attention_mask)\n",
        "\n",
        "          probs = torch.sigmoid(logit).detach().cpu().numpy().flatten()\n",
        "          batch_predictions = probs > self._prediction_threshold\n",
        "\n",
        "          for id, prediction in zip(ids, batch_predictions):\n",
        "            predictions[id] = prediction \n",
        "          \n",
        "      return df['id'].apply(lambda id: predictions[id]).tolist()\n",
        "      "
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ0LyfuA6_lj"
      },
      "source": [
        "** NearestNeiborClassifier Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "woWgCg3OPM1X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 185
        },
        "outputId": "c625f87c-1494-40e5-fd44-0cb49e7c70b4"
      },
      "source": [
        "test_nn_classifier = train_df.sample(1000)\n",
        "nearest_neibor_classifier = NearestNeiborClassifier(bert_model_name = BERT_MODEL_NAME, \n",
        "                                                     hidden_layer = -1,\n",
        "                                                     n_neighbors = 5,\n",
        "                                                    device = device)\n",
        "\n",
        "nearest_neibor_classifier.fit(test_nn_classifier, None)\n",
        "nearest_neibor_classifier.evaluate(test_nn_classifier)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 1000/1000 [00:12<00:00, 82.99it/s]\n",
            "100%|██████████| 1000/1000 [00:11<00:00, 83.53it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>fscore</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.906977</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.58209</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   precision    recall   fscore\n",
              "0   0.906977  0.428571  0.58209"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HN04zBCu7JpE"
      },
      "source": [
        "**DenseLayerOnBertModel Example:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hRO46jR2zGZv",
        "outputId": "67a62be2-a0ea-4fa1-80f2-6fdaa1daef8a"
      },
      "source": [
        "dense_model_classifier = DenseLayerOnBertModel(bert_model_name = BERT_MODEL_NAME, \n",
        "                                               hidden_layer = -1,\n",
        "                                               batch_size = 32,\n",
        "                                               prediction_threshold = 0.65,\n",
        "                                               device = device, \n",
        "                                               learnning_rate= 0.001,\n",
        "                                               number_of_epocs = 5,\n",
        "                                               verbose_eval = True)\n",
        "\n",
        "\n",
        "\n",
        "dense_model_classifier.fit(train_df.sample(500), validation_df.sample(100))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 500/500 [00:00<00:00, 614.47it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 100/100 [00:00<00:00, 551.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnTOnjJxwMQU"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rd2BRTnoszmH",
        "outputId": "a9d4c128-bb37-4ff0-aa14-eacab05e1972"
      },
      "source": [
        "BATCH_SIZE = 32\n",
        "PREDICTION_THRESHOLD = 0.65 \n",
        "N_NEIGHBORS = 3\n",
        "NUMBER_OF_EPOCS = 8\n",
        "EMBEDDING_LAYERS_TO_CHECK = range(0, 13) #[6, 7, 8, 9, 10 ,11 ,12]\n",
        "scores_dfs = []\n",
        "\n",
        "train_df = train_df.sample(2000)\n",
        "validation_df = validation_df.sample(200)\n",
        "\n",
        "\n",
        "for hidden_layer in EMBEDDING_LAYERS_TO_CHECK:\n",
        "\n",
        "  print('running on hidden layer {}'.format(hidden_layer))\n",
        "\n",
        "  dense_model_classifier = DenseLayerOnBertModel(bert_model_name = BERT_MODEL_NAME, \n",
        "                                               hidden_layer = hidden_layer,\n",
        "                                               batch_size = BATCH_SIZE,\n",
        "                                               prediction_threshold = PREDICTION_THRESHOLD,\n",
        "                                               device = device, \n",
        "                                               learnning_rate= 0.001,\n",
        "                                               number_of_epocs = NUMBER_OF_EPOCS,\n",
        "                                               verbose_eval = False)\n",
        "\n",
        "\n",
        "  dense_model_classifier.fit(train_df, None)\n",
        "  scores_df = dense_model_classifier.evaluate(validation_df)\n",
        "  scores_df['model'] = 'Deep'\n",
        "  scores_df['hidden_layer'] = hidden_layer\n",
        "  scores_dfs.append(scores_df)\n",
        "\n",
        "  nearestNeiborClassifier = NearestNeiborClassifier(bert_model_name = BERT_MODEL_NAME, hidden_layer = \n",
        "                                                      hidden_layer, n_neighbors = N_NEIGHBORS,\n",
        "                                                      device = device)\n",
        "\n",
        "\n",
        "  nearestNeiborClassifier.fit(train_df, None)\n",
        "  scores_df = nearestNeiborClassifier.evaluate(validation_df)\n",
        "\n",
        "  scores_df['model'] = 'NearestNeighbor'\n",
        "  scores_df['hidden_layer'] = hidden_layer\n",
        "  scores_dfs.append(scores_df)\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 626.57it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 564.38it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 81.83it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 79.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 625.65it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 561.40it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.12it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 79.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 624.03it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 558.76it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 81.72it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 78.41it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 625.38it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 560.17it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.16it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 78.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 628.98it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 564.26it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.36it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 76.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 615.34it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 564.30it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.32it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 80.62it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 616.77it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 567.96it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.66it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 78.70it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 7\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 619.94it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 568.49it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.37it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 78.86it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 628.79it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 554.03it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 80.55it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 78.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 629.45it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 564.15it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 83.18it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 80.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 627.12it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 567.99it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.78it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 78.15it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 617.53it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 563.06it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.49it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 77.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on hidden layer 12\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:03<00:00, 620.40it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "100%|██████████| 200/200 [00:00<00:00, 560.24it/s]\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "100%|██████████| 2000/2000 [00:24<00:00, 82.96it/s]\n",
            "100%|██████████| 200/200 [00:02<00:00, 79.36it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "roMLOKi3hVoD"
      },
      "source": [
        "results_df = pd.concat(scores_dfs)"
      ],
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4CGIALlZCILv"
      },
      "source": [
        "**Different Methods Results:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 855
        },
        "id": "yYm7qSDRhliH",
        "outputId": "477b7e43-1229-4ca9-c5cd-9f9605572be1"
      },
      "source": [
        "results_df.sort_values(by=[\"model\", 'hidden_layer'])"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>precision</th>\n",
              "      <th>recall</th>\n",
              "      <th>fscore</th>\n",
              "      <th>model</th>\n",
              "      <th>hidden_layer</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>Deep</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.357143</td>\n",
              "      <td>Deep</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.275862</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.340426</td>\n",
              "      <td>Deep</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.321429</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.391304</td>\n",
              "      <td>Deep</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.368421</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.378378</td>\n",
              "      <td>Deep</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>Deep</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>Deep</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.375000</td>\n",
              "      <td>Deep</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>Deep</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.516129</td>\n",
              "      <td>Deep</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.476190</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.512821</td>\n",
              "      <td>Deep</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.578947</td>\n",
              "      <td>0.611111</td>\n",
              "      <td>0.594595</td>\n",
              "      <td>Deep</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.484848</td>\n",
              "      <td>Deep</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.260870</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.551724</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.833333</td>\n",
              "      <td>0.277778</td>\n",
              "      <td>0.416667</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.875000</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.388889</td>\n",
              "      <td>0.482759</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>10</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>11</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>NearestNeibor</td>\n",
              "      <td>12</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   precision    recall    fscore          model  hidden_layer\n",
              "0   0.000000  0.000000  0.000000           Deep             0\n",
              "0   0.500000  0.277778  0.357143           Deep             1\n",
              "0   0.275862  0.444444  0.340426           Deep             2\n",
              "0   0.321429  0.500000  0.391304           Deep             3\n",
              "0   0.368421  0.388889  0.378378           Deep             4\n",
              "0   0.636364  0.388889  0.482759           Deep             5\n",
              "0   0.500000  0.444444  0.470588           Deep             6\n",
              "0   0.428571  0.333333  0.375000           Deep             7\n",
              "0   0.444444  0.444444  0.444444           Deep             8\n",
              "0   0.615385  0.444444  0.516129           Deep             9\n",
              "0   0.476190  0.555556  0.512821           Deep            10\n",
              "0   0.578947  0.611111  0.594595           Deep            11\n",
              "0   0.533333  0.444444  0.484848           Deep            12\n",
              "0   0.600000  0.166667  0.260870  NearestNeibor             0\n",
              "0   0.833333  0.277778  0.416667  NearestNeibor             1\n",
              "0   0.714286  0.277778  0.400000  NearestNeibor             2\n",
              "0   0.857143  0.333333  0.480000  NearestNeibor             3\n",
              "0   0.600000  0.333333  0.428571  NearestNeibor             4\n",
              "0   0.600000  0.333333  0.428571  NearestNeibor             5\n",
              "0   0.727273  0.444444  0.551724  NearestNeibor             6\n",
              "0   0.833333  0.277778  0.416667  NearestNeibor             7\n",
              "0   0.875000  0.388889  0.538462  NearestNeibor             8\n",
              "0   0.700000  0.388889  0.500000  NearestNeibor             9\n",
              "0   0.636364  0.388889  0.482759  NearestNeibor            10\n",
              "0   0.666667  0.444444  0.533333  NearestNeibor            11\n",
              "0   0.600000  0.333333  0.428571  NearestNeibor            12"
            ]
          },
          "metadata": {},
          "execution_count": 126
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "yQWCPMT7haGM",
        "outputId": "79f81cc9-bee1-4b6a-e0f6-4e28c67a8e33"
      },
      "source": [
        "sns.scatterplot(data=results_df, x=\"hidden_layer\", y=\"fscore\", hue=\"model\")"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.axes._subplots.AxesSubplot at 0x7f22afaa9d90>"
            ]
          },
          "metadata": {},
          "execution_count": 124
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2L1X2TEk98WN"
      },
      "source": [
        "\n",
        "**Methods:**\n",
        "\n",
        "I tried two methods, Nearest neighbor and Deep Neural network.\n",
        "\n",
        "I both methods I used pretrained bert model, bert-base-uncased.\n",
        "\n",
        "1. **Nearest neighbor classifier**:\n",
        "\n",
        "   Input: Sentense embedding, mean on the words vectors from hidden layer..\n",
        "\n",
        "   I didnt try to optimize the hyper paramaters like number of neighbors.\n",
        "\n",
        "\n",
        "2. **Deep Neural network on top of Bert hidden layers**:\n",
        "  \n",
        "  I freezed bert weights, I selected one of its hidden layers and I calculated the mean of the words vectors (sentense embeddings). \n",
        "\n",
        "  I insered the sentense embeddings to fully connected layer that output logits \n",
        "\n",
        "  Optimizer: Adam with learnning rate 0.001\n",
        "\n",
        "  Loss function: BCEWithLogitsLoss - with postive weights, since the dataset is imbalanced.\n",
        "\n",
        "  I didnt try to optimize hyper paramaters, optimizers, loss function etc.\n",
        "\n",
        "\n",
        "**Challnges:**\n",
        "\n",
        "The dataset is highly imbalanced and I could try diffrent methods to deal with it. (like undersampling, oversampling)\n",
        "\n",
        "The dataet is large and trainning on all of it in colab would take long time so \n",
        "I sampled smaller dataset and trained for short time (10 epocs).\n",
        "\n",
        "\n",
        "**Conclussions:**\n",
        "  \n",
        " I trained on small dataset and for low number of epocs so its difficult to jump intop conclusions but I will try.\n",
        "\n",
        "\n",
        "I used the fscore to analyze the results.\n",
        "\n",
        "\n",
        "In both model we can see clearly that the early layers (~0-5) give worse results than later layers (6-12).\n",
        "\n",
        "\n",
        "In nearest neighbor it look like its the best to use some middle layer 6-8.\n",
        "\n",
        "\n",
        "In the Deep model the last layers 8-12 are much better than earlier layers, and the best results is achived when using layer 11.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    }
  ]
}